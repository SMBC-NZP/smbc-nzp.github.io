---
output: 
  html_document:
    self_contained: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<head>
<link rel="stylesheet" href=""N:/Capacity Building and Academic Programs/SI-Mason Grad & Prof Training/Individual Courses--Folders/Bird Migration/2018/Materials/Distance">
<link rel="stylesheet" type="text/css" href="styles.css">
</head>

# House Wren distance sampling analysis
<p><i>Joe Kolowski, Ph.D.</i></p>

## Project Description and Context

<img style="float:right" src="House_Wren_b57-13-170_l_crop.png" />

In this module, we will import point transect data on individual house wrens (*Troglodytes aedon*). The data were collected from 155 points, with 14-16 points within each of 10 study blocks. The blocks were 16 hectares each and were situated in riparian vegetation along 30km of South Platte River bottomland near Crook, Colorado, USA. The data were collected by 4 observers, who each visited each point. The study is used as an illustrative example in section 8.6 of the Buckland et al. (2001) text and this is one of the Sample Projects provided with the program DISTANCE. Here we will conduct a similar analysis of this data set using the R package "Distance". 

##Exercise Objectives
  * Become familiar with data import and formatting for point transect data when using this package 
  * Learn how to explore your data visually to inform the analytical approach and identify potential issues
  * Practice basic Conventional Distance Sampling (CDS) analysis to estimate density
  * Practice use of a covariate to inform detection function modeling, and the use of model selection to compare evidence for hypotheses about detection processes
  * Learn to interpret results from a distance sampling analysis and use these results to make conclusions 
  
###Data Import and Exploration

With the package `Distance`, we can bring in our data as a single "flatfile" format. This is the same format we can use to import into the Windows program DISTANCE. A .csv format is generally the most straightforward way to import data into R. Using the `head` and `summary` functions, you can see that we have 5 columns of data, indicating our block ID (0-9), the area of each block, which are all 16 hectares, our point IDs (1-16), our distance of each observation in meters, the ID of each observer, and the total effort at each point, which was 4 repeats. We can also see our maximum distance observed was 90 meters.
```{r}
wrendata <- 
  read.csv('https://www.dropbox.com/s/vfehqw6df1gupuz/HouseWrenData.csv?dl=1')

head(wrendata)

summary(wrendata)
```

The R package we will be using requires us to name some of our critical data columns in a particular way. Let's do this now. "Region"" refers to a stratum, which may be a survey period, or a geographic region. "Sample"" refers to a point or transect.
```{r}
names(wrendata) <-
  c("Region.Label",
    "Area",
    "Sample.Label",
    "distance",
    "Obs",
    "Effort")
```

We should also make sure R is reading our covariates as we want. We can see that observer and block are read by R as integers, but these are really categorical variables, so we need to change them to factors in R.
```{r}
str(wrendata)

wrendata$Region.Label <- 
  as.factor(wrendata$Region.Label)

wrendata$Obs <- 
  as.factor(wrendata$Obs)

str(wrendata)
```

Now let's look at our data, and some of our potential covariates. First let's just look at the distance data. Note that we can play around with how our histograms are viewed by changing the breaks for the bins. In the third example, you can see we can set custom bin sizes that may help make the pattern more clear, and address some potential issues with the data. Many bird studies will actually analyze their data in specific distance bins, or even collect the data that way. We'll show later how to indicate that your data were collected in bins, and how to bin the data after it was collected. Here this is simply for visualization purposes. Note that using 20 equal-sized bins we can see some potential rounding errors, with unexpectedly high numbers of observations in the bins at 10m and 20m. 
```{r}
par(mfrow=c(1,3))

hist(wrendata$distance,
     main="Default Binning",
     xlab = "Distance")

hist(wrendata$distance,
     breaks = seq(0,100, length.out = 20),
     main="20 equal size bins",
     xlab = "Distance")

hist(wrendata$distance,
     breaks = c(0.0, 7.5, 12.5, 17.5, 22.5, 27.5, 32.5, 42.5, 62.5, 92.5),
     main="Custom Bins",
     xlab = "Distance")
```

Now let's investigate our potential strata and covariates, observer and block, relative to observed distances. We can see that the distance observations across the 4 observers are fairly consistent, with perhaps the exception of observer 4, who tends to record birds at further distances.
```{r}
par(mfrow=c(1,2))

boxplot(
  formula = wrendata$distance~wrendata$Obs, 
  xlab="Observer",
  ylab="Distance(m)")

boxplot(
  formula = wrendata$distance~wrendata$Region.Label, 
  xlab="Block", 
  ylab="Distance(m)")
```


###Data Analysis - Single Pooled Detection Function, for all Observers and all Blocks
We can approach our analysis here in a number of ways, since our data were collected in 10 different geographic regions (i.e. Blocks) and we may want to investigate whether detection processes vary across those regions. In addition, we may want to try to account for potential variation across our 4 observers. Let's first start with a basic analysis that essentially ignores all this potential variation. And we'll begin by calling the package we need. 
```{r, warning = FALSE, message = FALSE}
library(Distance)
```

<p>We'll begin the analysis by testing 4 different key function and adjustment combinations that tend to do well with animal observation data. This is meant to identify a detection function shape that best fits our collected distance data. As is generally good practice, we will truncate the furthest 10% of observations. One could also view the histograms above, and decide on a custom truncation value. 
<p>*Note also that it is here, in the use of the `ds` function, that one would indicate that their data was collected in bins and/or that it should be analyzed in bins. You would use the argument `cutpoints` and provide a vector of bin start and end distances.*

```{r, message = F, warning = F}
wren.hn.herm <- 
  ds(wrendata, 
     truncation="10%", 
     transect="point",
     key="hn", 
     adjustment="herm", 
     convert.units = 0.01)

wren.hn.cos <- 
  ds(wrendata, 
     truncation="10%", 
     transect="point",
     key="hn", 
     adjustment="cos", 
     convert.units = 0.01)

wren.uni.cos <- 
  ds(wrendata, 
     truncation="10%", 
     transect="point",
     key="unif", 
     adjustment="cos", 
     convert.units = 0.01)

wren.haz.simp <- 
  ds(wrendata, 
     truncation="10%", 
     transect="point",
     key="hr", 
     adjustment="poly", 
     convert.units = 0.01)
```
*This conversion value of 0.01 is to get the density results in birds per hectare (100x100m). This can be a bit confusing, but in practice here your sampled area (in this case the sizes of your circles, determined by distances recorded in meters) will be multipled by your conversion number. And we want this conversion to match our unit for area, which is hectares. To get meters to match hectares (100m square) we need to bring meters to that same scale, therefore multiply by 0.01.*

We can review the AIC values of these 4 models by using the function `AIC`.
```{r}
AIC(wren.hn.herm)
AIC(wren.hn.cos)
AIC(wren.haz.simp)
AIC(wren.uni.cos)
```

We can also use the `summarize_ds_models` function to look at a summary table of the model results like this:
```{r}
summary1 <- 
  summarize_ds_models(
    wren.haz.simp,
    wren.uni.cos,
    wren.hn.cos,
    wren.hn.herm,
    output = "plain")

summary1
```
Three of these models are quite close in AIC value. We can plot them to visually inspect the fit of the detection function. We must plot as a probability density function (pdf), as opposed to the default setting, because these are circular points. Note that the `plot` function automatically recognizes that our model results are a dsmodel object, and creates an essentially customized plot with many useful default settings.
```{r}
par(mfrow=c(1,3))

plot(wren.hn.cos,
     main="Wren data pooled, \nhalf-normal cosine detection function", 
     pdf = T)

plot(wren.haz.simp,
     main="Wren data pooled, \nhazard-rate simple polynomial detection function",
     pdf = T)

plot(wren.uni.cos,
     main="Wren data pooled, \nuniform cosine detection function",
     pdf = T)
```

All three seem to be very similar in visual fit, though the default binning may not be ideal for assessing fit. We can manually alter the bins as we did for the histograms above. 
```{r, message = F}
par(mfrow=c(1,3))

plot(wren.hn.cos,
     main="Wren data pooled, \nhalf-normal cosine detection function", 
     breaks = c(0.0, 7.5, 12.5, 17.5, 22.5, 27.5, 32.5, 42.5), 
     pdf = T)

plot(wren.haz.simp,
     main="Wren data pooled, \nhazard-rate simple polynomial detection function",
     breaks = c(0.0, 7.5, 12.5, 17.5, 22.5, 27.5, 32.5, 42.5), 
     pdf = T)

plot(wren.uni.cos,
     main="Wren data pooled, \nuniform cosine detection function", 
     breaks = c(0.0, 7.5, 12.5, 17.5, 22.5, 27.5, 32.5, 42.5),
     pdf = T)
```

Let's proceed to the next step of comparing some diagnostic values. Although some of these are produced automatically by the `summarize_ds_models` function, we can get a more complete diagnostic summary with the `ddf.gof` function. This function takes a dsmodel object as its argument, and produces a QQ plot as well as a series of diagnostic tests. The most favored, is the Cramer-von-Mises test statistic, which uses the real data (as opposed to binned data like the chi-square test) and gives priority to the fit of the detection function at small distances. From the list created from the `ddf.gof` function, we will pull out the results of this test. The 'W' shown is the test statistic and a low p-value would indicate a significant departure from the expected detection function. Here we again only see minor variations in the plots, and the Cramer-von-Mises test.

```{r}
par(mfrow=c(1,3))

fit.wren.hn.cos <- 
  ddf.gof(wren.hn.cos$ddf)

fit.wren.uni.cos <- 
  ddf.gof(wren.uni.cos$ddf)

fit.wren.haz.simp <- 
  ddf.gof(wren.haz.simp$ddf)

fit.wren.hn.cos$dsgof$CvM

fit.wren.uni.cos$dsgof$CvM

fit.wren.haz.simp$dsgof$CvM
```

Let's move forward looking at the results of the top model then, just to summarize and interpret the results. One can easily use `summary(wren.haz.simp)` to view a summary of the input data and all the relevant results. You can also pick and choose elements of the model results to focus on. Here we are pulling out some elements of the dsmodel object. I'm using the `kable` function from the `knitr` package just to create easily viewable tables in a markdown file, but you can simply call the object directly (e.g. `wren.haz.simp$dht$individuals$D`). Although results are shown for each region/block, we are here just looking at the total value for density and abundance. Our density value here is `r wren.haz.simp$dht$individuals$D[11,2]` birds per hectare.
```{r, message = F}
library(knitr)
```

```{r}
kable(wren.haz.simp$dht$individuals$summary,format="markdown")
kable(wren.haz.simp$dht$individuals$D,format="markdown")
```


###Data Analysis - Observer as a Covariate
In the analysis above, we assumed there was a single detection function, for all observers and all blocks. But we know from exploring our data that there seems to be some difference across our 4 observers. If this is true, then pooling the detection function across observers is not ideal, and we should attempt to create separate detection functions for each observer. To do this we will use Observer as a covariate. In this approach, a single key function and adjustment framework is used, but the parameters of the function can be independently assigned depending on observer. We now specify a model formula in our `ds` function, in this case indicating that the detection function varies by data in the column `Obs`. 
```{r, message = F, warning = F}
wren.haz.simp.obs <- 
  ds(wrendata, 
     truncation="10%", 
     transect="point",
     key="hr", 
     adjustment="poly", 
     convert.units = 0.01, 
     formula = ~Obs)
```

The AIC value here is well below the AIC of the best model above, which ignored the influence of the observers.
```{r}
AIC(wren.haz.simp.obs)
```

We can plot the results using a simple `plot` command. Since the model included a covariate, the plot will automatically show the detection function for each level of a factor covariate. We can also use the `ddf.gof` function to get some diagnostics. In this case we are using some additional code to put the results of the Cramer von-Mises text on the QQPlot. There is no indication here that we do not have a good fit to our data. Regarding the observers, we do see that one observer stands out, showing relatively low detection near the point center, and much more frequent detections at far distances.
```{r}
par(mfrow=c(1,2))

plot(wren.haz.simp.obs,
     main="Wren Data, \ndetection function with Observer as covariate",
     pdf = T)

covar.fit <- 
  ddf.gof(wren.haz.simp.obs$ddf)

message <- 
  paste(
    "Cramer von-Mises W=",
    round(covar.fit$dsgof$CvM$W,3), 
    "\nP=", round(covar.fit$dsgof$CvM$p,3))

text(0.6, 0.1, message, cex=0.8)
```

Looking at the summary of the results, we see that the model coefficients are set up to view Observer 1 as the baseline. With this reference, Observer 4 is clearly different from Observer 1. Having used observer as a covariate, we now have a less-biased estimate of density across the study area, though the final estimate of density has not changed much, we do have a lower coefficient of variation. Our new density value is `r round(wren.haz.simp.obs$dht$individuals$D[11,2],2)` birds per hectare with a coefficient of variation of `r round(wren.haz.simp.obs$dht$individuals$D$cv[11],2) *100`%.
```{r}
summary(wren.haz.simp.obs)
```

###Data Analysis - Observer as a Stratum
We could also treat each observer as completely separate, having separate detection functions entirely for each observer. This is what we might elect to do with Block, or if our study area was divided into a few main regions or habitat types. With this approach, we would not treat observer then as a covariate for our function parameters, but rather estimate 4 totally unique detection functions. We may have reason to think that observer 4 is so different, that we need to take this approach. 

We will first need to create 4 new dataframes. Because the point that resulted in no bird observations from any observer listed "NA" as the observer, we need to bring these along in every dataset. We also need to reset the effort level to 1 for each separate data set.

```{r}
wrendataObs1 <- 
  subset(wrendata,
         wrendata$Obs=="1" | is.na(wrendata$Obs)
         )

wrendataObs1$Effort <- 1

wrendataObs2 <- subset(wrendata, wrendata$Obs=="2" | is.na(wrendata$Obs))
wrendataObs2$Effort <- 1

wrendataObs3 <- subset(wrendata, wrendata$Obs=="3" | is.na(wrendata$Obs))
wrendataObs3$Effort <- 1

wrendataObs4 <- subset(wrendata, wrendata$Obs=="4" | is.na(wrendata$Obs))
wrendataObs4$Effort <- 1
```

Now we can test which combination of key function and adjustments work best for each separate dataset. We'll focus just on the 4 most common detection function models.
```{r, message = F, warning = F}
wren.hn.herm.obs1 <-
  ds(
    wrendataObs1,
    truncation = "10%",
    transect = "point",
    key = "hn",
    adjustment = "herm",
    convert.units = 0.01
  )

wren.hn.cos.obs1 <-
  ds(
    wrendataObs1,
    truncation = "10%",
    transect = "point",
    key = "hn",
    adjustment = "cos",
    convert.units = 0.01
  )

wren.unif.cos.obs1 <-
  ds(
    wrendataObs1,
    truncation = "10%",
    transect = "point",
    key = "unif",
    adjustment = "cos",
    convert.units = 0.01
  )

wren.haz.simp.obs1 <-
  ds(
    wrendataObs1,
    truncation = "10%",
    transect = "point",
    key = "hr",
    adjustment = "poly",
    convert.units = 0.01
  )

wren.hn.herm.obs2 <-
  ds(
    wrendataObs2,
    truncation = "10%",
    transect = "point",
    key = "hn",
    adjustment = "herm",
    convert.units = 0.01
  )

wren.hn.cos.obs2 <-
  ds(
    wrendataObs2,
    truncation = "10%",
    transect = "point",
    key = "hn",
    adjustment = "cos",
    convert.units = 0.01
  )

wren.unif.cos.obs2 <-
  ds(wrendataObs2,
     truncation = "10%",
     transect = "point",
     key = "unif",
     adjustment = "cos",
     convert.units = 0.01)

wren.haz.simp.obs2 <-
  ds(
    wrendataObs2,
    truncation = "10%",
    transect = "point",
    key = "hr",
    adjustment = "poly",
    convert.units = 0.01
  )

wren.hn.herm.obs3 <- 
  ds(
    wrendataObs3,
    truncation = "10%",
    transect = "point",
    key = "hn",
    adjustment = "herm",
    convert.units = 0.01
  )

wren.hn.cos.obs3 <-
  ds(
    wrendataObs3,
    truncation = "10%",
    transect = "point",
    key = "hn",
    adjustment = "cos",
    convert.units = 0.01
  )

wren.unif.cos.obs3 <-
  ds(
    wrendataObs3,
    truncation = "10%",
    transect = "point",
    key = "unif",
    adjustment = "cos",
    convert.units = 0.01
  )

wren.haz.simp.obs3 <-
  ds(
    wrendataObs3,
    truncation = "10%",
    transect = "point",
    key = "hr",
    adjustment = "poly",
    convert.units = 0.01
  )

wren.hn.herm.obs4 <-
  ds(
    wrendataObs4,
    truncation = "10%",
    transect = "point",
    key = "hn",
    adjustment = "herm",
    convert.units = 0.01
  )

wren.hn.cos.obs4 <-
  ds(
    wrendataObs4,
    truncation = "10%",
    transect = "point",
    key = "hn",
    adjustment = "cos",
    convert.units = 0.01
  )

wren.unif.cos.obs4 <- 
  ds(
    wrendataObs4,
    truncation = "10%",
    transect = "point",
    key = "unif",
    adjustment = "cos",
    convert.units = 0.01
  )

wren.haz.simp.obs4 <- 
  ds(
    wrendataObs4,
    truncation = "10%",
    transect = "point",
    key = "hr",
    adjustment = "poly",
    convert.units = 0.01
  )
```

We can now sum the AIC values across the best models for each observer. This AIC is comparable then to our other AIC values.
```{r}
full.aic <- AIC(wren.hn.cos.obs1) + 
  AIC(wren.hn.cos.obs2) + 
  AIC(wren.haz.simp.obs3) + 
  AIC(wren.unif.cos.obs4)

full.aic
```

Our new summed AIC value is now 5507.155. This approach then, where each observer gets their own detection function, is not worth the extra complexity, and the better approach is to use observer as a covariate. We can still look at the detection functions for each observer though, just out of curiosity. Here I am holding the limits of the x and y axes constant to help in comparing across figures.
```{r}
par(mfrow=c(2,2))

plot(wren.haz.simp.obs1,
     main="Wren Data, \ndetection function for Observer 1, \nHN-COS",
     xlim = c(0,50),
     ylim = c(0,0.08),
     pdf = T)

plot(wren.haz.simp.obs2,
     main="Wren Data, \ndetection function for Observer 2, \nHN-COS",
     xlim = c(0,50),
     ylim = c(0,0.08),
     pdf = T)

plot(wren.haz.simp.obs3,
     main="Wren Data, \ndetection function for Observer 3 \nHR-POLY",
     xlim = c(0,50),
     ylim = c(0,0.08),
     pdf = T)

plot(wren.haz.simp.obs4,
     main="Wren Data, \ndetection function for Observer 4 \nUNIF-COS",
     xlim = c(0,50),
     ylim = c(0,0.08),
     pdf = T)
```

### Analysis Removing Observer 4

Finally, if we think observer 4 may not have collected the data properly, we can exclude observer 4, and rerun the analysis, with observer as a covariate. We will first create a new dataset, removing all observations from observer 4, and changing the effort level to 3. 
```{r}
wrendata_new <- 
  subset(wrendata, wrendata$Obs!="4")

wrendata_new$Effort <- 3
```
We would probably at this point re-assess what the best detection function might be for these three observers, but let's move forward with the hazard rate, simply polynomial.
```{r, message = F, warning = F}
wren.haz.simp.no4 <- 
  ds(wrendata_new, 
     truncation="10%", 
     transect="point",
     key="hr", 
     adjustment="poly", 
     convert.units = 0.01, 
     formula = ~Obs)
```
At this point the AIC value is no longer comparable, since we are working with a different data set, but we can now look at the density and variance to compare with our previous estimate. 
```{r}
summary(wren.haz.simp.no4)
```
We can see that our new density estimate of `r round(wren.haz.simp.no4$dht$individuals$D[11,2],2)` birds per hectare is much higher than our original estimate with no covariates, `r round(wren.haz.simp$dht$individuals$D[11,2],2)`, and our estimate when we accounted for observer, but kept observer 4 in the data `r round(wren.haz.simp.obs$dht$individuals$D[11,2],2)`. Our new estimate is much higher now, because the effective detection distance (EDD) estimated for observer 4 was likely artificially high, meaning we were assuming observer 4 covered a lot more area than he/she actually did. This falsely lowered the estimated density.